{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38c900d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import langchain\n",
    "import ollama\n",
    "import os\n",
    "import streamlit\n",
    "import textract # appears broken for pdf on windows due to shell call\n",
    "import pypdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ff7fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables to point at documents\n",
    "document_1_location = \"C:/Users/justi/Documents/GitHub/SmallProjects/documents/Scotland's Wild Deer_ A National Approach 2015-2020 Priorities.pdf\"\n",
    "document_2_location = \"C:/Users/justi/Documents/GitHub/SmallProjects/documents/deer-management-on-scotlands-national-forest-estate.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9834762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(document_location: str) -> str:\n",
    "    extension = os.path.splitext(document_location)[-1].lower()\n",
    "    if extension == '.txt':\n",
    "        with open(document_location) as document:\n",
    "            full_text = document.read()\n",
    "    elif extension == '.pdf':\n",
    "        # https://pypdf.readthedocs.io/en/stable/user/extract-text.html\n",
    "        reader = pypdf.PdfReader(document_location)\n",
    "        number_of_pages = len(reader.pages)\n",
    "        full_text = \"\"\n",
    "        # extract text page by page\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            full_text = full_text + text\n",
    "        # remove linebreaks\n",
    "        full_text = full_text.replace('\\n', ' ')\n",
    "    else:\n",
    "        # try to use textract - warning may be buggy - certainly fails pdfs\n",
    "        # https://textract.readthedocs.io/en/stable/\n",
    "        full_text = textract.process(document_location)\n",
    "    return full_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc6002fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set up our ChromaDB NON-PERSISTENT database\n",
    "# # https://docs.trychroma.com/getting-started\n",
    "# chroma_client = chromadb.Client()\n",
    "# # while we do not intend to persist this chrmoadb, lets use a careful creation function\n",
    "# collection = chroma_client.get_or_create_collection(name=\"comparison_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e7d54d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract our documents to plain text\n",
    "text_1 = extract_text(document_1_location)\n",
    "text_2 = extract_text(document_2_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6839b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # chunk our text to keep it inside the context window\n",
    "# # https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter/\n",
    "# text_splitter = langchain.text_splitter.RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=2000, # characters not words\n",
    "#     chunk_overlap=100,\n",
    "#     length_function=len,\n",
    "# #     is_separator_regex=False,\n",
    "# )\n",
    "# text_1_chunks = text_splitter.split_text(text_1)\n",
    "# text_2_chunks = text_splitter.split_text(text_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b63eba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # store the chunks of one document in the database\n",
    "# for index, chunk in enumerate(text_1_chunks):\n",
    "#     collection.add(\n",
    "#         documents=[chunk],\n",
    "#         ids=[str(index)]\n",
    "#     )\n",
    "#     print(index, chunk)\n",
    "    \n",
    "# # stores a tarball at C:\\Users\\justi\\.cache\\chroma\\onnx_models\\all-MiniLM-L6-v2\\onnx.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1373d054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # now lets perform a comparison between chunks document 2 and the most similar chunks of document 1 as found in the database\n",
    "# answers = []\n",
    "# for chunk in text_2_chunks:\n",
    "#     print(\"CHUNK+++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "#     print (chunk)\n",
    "#     # retrieve the most similar chunk from the database\n",
    "#     results = collection.query(\n",
    "#         query_texts=[chunk],\n",
    "#         n_results=1\n",
    "#     )\n",
    "#     print(results)\n",
    "#     # join the top 2 results?\n",
    "#     result = results['documents'][0][0] #+ \" \" + results[1]\n",
    "#     print(\"MATCHED-------------------------------------------\")\n",
    "#     print(result)\n",
    "#     query = f\"\"\"\n",
    "#         Please describe 3 similarities and 3 differences between the following two passages. \n",
    "#         Provide these answers as two numbered lists labelled SIMILARITIES and DIFFERENCES.\n",
    "        \n",
    "#         Passage 1:\n",
    "#         {chunk}\n",
    "        \n",
    "#         passage 2:\n",
    "#         {result}\n",
    "#         \"\"\"\n",
    "#     response = ollama.chat(model='llama3:8b', messages=[\n",
    "#       {\n",
    "#         'role': 'user',\n",
    "#         'content': query,\n",
    "#       },\n",
    "#     ])\n",
    "#     answer = response['message']['content']\n",
    "#     print(\"ANSWER-------------------------------------------\")\n",
    "#     print(answer)\n",
    "#     answers.append(answer)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ef567fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "935fd103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def recursive_summariser(\n",
    "#     text: str,\n",
    "#     model: str ='llama3:8b',\n",
    "#     input_chunk_tokens: int = 5000, # characters not tokens\n",
    "#     chunk_overlap: int = 20,\n",
    "#     compression_ratio: float = 5.0,\n",
    "#     max_summary_length: int = 5000, # characters not tokens\n",
    "# ):\n",
    "#     input_length = len(text)\n",
    "#     print(\"Input_text_length: \" + str(len(text)))\n",
    "#     # see if we can achieve our desired compressions ratio with a less agressive compression ratio\n",
    "#     required_compression = input_length / max_summary_length\n",
    "#     if required_compression < compression_ratio:\n",
    "#         # this could be a higher compression ration than desired, but unlikely\n",
    "#         # it should ensure sufficient compression on late rounds to make the last round of compression close to optimal\n",
    "#         compression_ratio = required_compression * 1.3 \n",
    "#         print(f\"changed compressions ratio to : {compression_ratio}\")\n",
    "#     # split the passage based on the token length\n",
    "#     text_splitter = langchain.text_splitter.RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=input_chunk_tokens, # characters not words\n",
    "#         chunk_overlap=chunk_overlap,\n",
    "#         # add full stops and commas to the separators to try to get the most sensible splitting \n",
    "#         separators=[\"\\n\\n\", \"\\n\", \".\", \",\", \" \", \"\"],\n",
    "#         length_function=len,\n",
    "#     #     is_separator_regex=False,\n",
    "#     )\n",
    "# #     text_token_count = text_splitter.count_tokens(text)\n",
    "# #     print(\"Starting tokens for this recursion:\" + str(text_token_count))\n",
    "#     text_chunks = text_splitter.split_text(text)\n",
    "#     # handle the edge case that we are already down to a chunk smaller than the chunk size\n",
    "#     if len(text_chunks) <= 1:\n",
    "#         return text\n",
    "#     # summarise each chunk in turn\n",
    "#     responses = []\n",
    "#     for chunk in text_chunks:\n",
    "#         word_count = len(chunk.split(\" \"))\n",
    "#         desired_word_count = int(word_count//compression_ratio)\n",
    "#         print(f\"chunk length: {len(chunk)}, word count: {word_count}, desired word count: {desired_word_count}\")\n",
    "#         query = f\"\"\"\n",
    "# You are a professional document summariser\n",
    "# Please summarise the following text from {word_count} words, down to {desired_word_count} words. \n",
    "# Be careful to retain as much of the overall meaning of the text as possible in your summary. \n",
    "# Include nothing but the summary in your reply. Do not say how many words it is summarised to or mention that it is a summary.\n",
    "\n",
    "# Text:\n",
    "# {chunk}\n",
    "# \"\"\"\n",
    "#         response = ollama.chat(model='llama3:8b', messages=[\n",
    "#           {\n",
    "#             'role': 'user',\n",
    "#             'content': query,\n",
    "#           },\n",
    "#         ])\n",
    "#         response_text = response['message']['content']\n",
    "#         responses.append(response_text)\n",
    "    \n",
    "#     # concatenate the chunks\n",
    "#     summary = \"\\n\".join(responses)\n",
    "#     print(\"Summary length in characters: \" + str(len(summary)))\n",
    "    \n",
    "#     # check summary length\n",
    "#     summary_splitter = langchain.text_splitter.RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=max_summary_length, # characters not words\n",
    "#         chunk_overlap=0,\n",
    "#         length_function=len,\n",
    "#     #     is_separator_regex=False,\n",
    "#     )\n",
    "# #     summary_token_count = summary_splitter.count_tokens(summary)\n",
    "#     summary_chunks = text_splitter.split_text(summary)\n",
    "#     # if summary is short enough return it\n",
    "#     if len(summary_chunks) <= 1:\n",
    "#         return summary\n",
    "#     # otherwise call recursive_summariser on the summary\n",
    "#     else:\n",
    "#         return recursive_summariser(summary)\n",
    "    \n",
    "# def tidy_text(text: str):\n",
    "#     query = f\"\"\"\n",
    "# You are a professional copy editor\n",
    "# Please edit the following text ensuring that it maintains a consistent grammatical style throughout.\n",
    "# Remove any references to summarisation or word counts.\n",
    "# Ensure you preserve all the factual meaning, but remove any incoherent text.\n",
    "# Break the text into meaningful paragraphs.\n",
    "# Your edited text should be approximately the same number of words as the original.\n",
    "# Your edited text must be written in British rather than American English.\n",
    "# Include nothing but the edited text in your reply. Do not mention that it is edited.\n",
    "\n",
    "# Text:\n",
    "# {text}\n",
    "#             \"\"\"\n",
    "#     edited_summary = ollama.chat(model='llama3:8b', messages=[\n",
    "#       {\n",
    "#         'role': 'user',\n",
    "#         'content': query,\n",
    "#       },\n",
    "#     ])\n",
    "#     return edited_summary['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af9ff83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_summary_1 = recursive_summariser(text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2297e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(raw_summary_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bcc00ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(raw_summary_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9436950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tidy_summary_1 = tidy_text(raw_summary_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64d180c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tidy_summary_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3731ea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(tidy_summary_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f40e901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(text_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cefb485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_summary_2 = recursive_summariser(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e711c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(raw_summary_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "221880d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(raw_summary_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "148c873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tidy_summary_2 = tidy_text(raw_summary_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a14d8c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_summaries(\n",
    "    summary_1: str,\n",
    "    summary_2: str,\n",
    "    model:str = 'llama3:8b',\n",
    "    max_length:int = 10000,\n",
    "):\n",
    "    query = f\"\"\"\n",
    "Please describe the similarities and differences between the following two passages. \n",
    "Focus on similarites and differences of content and emphasis, rather than style or phrasing.\n",
    "Keep your answer to a maximum of 1000 words in total.\n",
    "Provide your answers as two numbered lists labelled SIMILARITIES and DIFFERENCES.\n",
    "\n",
    "Passage 1:\n",
    "{summary_1}\n",
    "\n",
    "passage 2:\n",
    "{summary_2}\n",
    "\"\"\"\n",
    "    if len(query) > max_length:\n",
    "        print(\"Warning query may exceed context window\")\n",
    "    response = ollama.chat(model=model, messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': query,\n",
    "      },\n",
    "    ])\n",
    "    answer = response['message']['content']\n",
    "    return(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50db1f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning query may exceed context window\n",
      "Scotland's Deer Management Strategy outlines the comprehensive approach towards managing deer populations within Scotland. The strategy emphasizes sustainable practices, conservation of biodiversity, and enhancement of landscapes. \n",
      "\n",
      "The strategy includes various management practices such as monitoring wildlife populations and their habitats, implementing measures to prevent overgrazing and protect ground vegetation. It also focuses on promoting public engagement through education programs about deer ecology and the importance of sustainable forestry practices. Moreover, it highlights the benefits of healthy deer populations for maintaining a balanced ecosystem.\n"
     ]
    }
   ],
   "source": [
    "comparison = compare_summaries(text_1, text_2, 'herald/phi3-128k:latest')\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93b3d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_strategies(\n",
    "    summary_1: str,\n",
    "    summary_2: str,\n",
    "    model:str = 'llama3:8b',\n",
    "    max_length:int = 10000,\n",
    "):\n",
    "    query = f\"\"\"\n",
    "You are an enviromental strategy expert.\n",
    "Please describe the similarities and differences in the strategies outlined by the following passages. \n",
    "Focus on similarites and differences of strategy, rather than style, tone or phrasing.\n",
    "Provide your answers as two numbered lists labelled SIMILARITIES and DIFFERENCES.\n",
    "\n",
    "Passage 1:\n",
    "{summary_1}\n",
    "\n",
    "passage 2:\n",
    "{summary_2}\n",
    "\"\"\"\n",
    "    if len(query) > max_length:\n",
    "        print(\"Warning query may exceed context window\")\n",
    "    response = ollama.chat(model=model, messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': query,\n",
    "      },\n",
    "    ])\n",
    "    answer = response['message']['content']\n",
    "    return(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4496f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning query may exceed context window\n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "model 'herald/phi3-128k:latest' not found, try pulling it first",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m strategy_comparison \u001b[38;5;241m=\u001b[39m compare_strategies(text_1, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mherald/phi3-128k:latest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(strategy_comparison)\n",
      "Cell \u001b[1;32mIn[26], line 21\u001b[0m, in \u001b[0;36mcompare_strategies\u001b[1;34m(summary_1, summary_2, model, max_length)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(query) \u001b[38;5;241m>\u001b[39m max_length:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning query may exceed context window\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m response \u001b[38;5;241m=\u001b[39m ollama\u001b[38;5;241m.\u001b[39mchat(model\u001b[38;5;241m=\u001b[39mmodel, messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     22\u001b[0m   {\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: query,\n\u001b[0;32m     25\u001b[0m   },\n\u001b[0;32m     26\u001b[0m ])\n\u001b[0;32m     27\u001b[0m answer \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(answer)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\langchain\\Lib\\site-packages\\ollama\\_client.py:177\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, model, messages, stream, format, options, keep_alive)\u001b[0m\n\u001b[0;32m    174\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;241m:=\u001b[39m message\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    175\u001b[0m     message[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [_encode_image(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_stream(\n\u001b[0;32m    178\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    179\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    180\u001b[0m   json\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: model,\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m: messages,\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m'\u001b[39m: stream,\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mformat\u001b[39m,\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m'\u001b[39m: options \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeep_alive\u001b[39m\u001b[38;5;124m'\u001b[39m: keep_alive,\n\u001b[0;32m    187\u001b[0m   },\n\u001b[0;32m    188\u001b[0m   stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    189\u001b[0m )\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\langchain\\Lib\\site-packages\\ollama\\_client.py:97\u001b[0m, in \u001b[0;36mClient._request_stream\u001b[1;34m(self, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_stream\u001b[39m(\n\u001b[0;32m     92\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m     94\u001b[0m   stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     95\u001b[0m   \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     96\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], Iterator[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[1;32m---> 97\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\langchain\\Lib\\site-packages\\ollama\\_client.py:73\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m   response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[1;31mResponseError\u001b[0m: model 'herald/phi3-128k:latest' not found, try pulling it first"
     ]
    }
   ],
   "source": [
    "strategy_comparison = compare_strategies(text_1, \"help\", 'herald/phi3-128k:latest')\n",
    "print(strategy_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c4eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c900d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import ollama\n",
    "import os\n",
    "import pypdf\n",
    "import textract # appears broken for pdf on windows due to shell call\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff7fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables to point at documents\n",
    "# Document 1 is from: https://www.nature.scot/sites/default/files/2017-06/A2003405%20-%20Scotland%27s%20Wild%20Deer_%20A%20National%20Approach%202015-2020%20Priorities%234.pdf\n",
    "document_1_location = \"C:/Users/justi/Documents/GitHub/SmallProjects/documents/Scotland's Wild Deer_ A National Approach 2015-2020 Priorities.pdf\"\n",
    "# Document 2 is from: https://forestryandland.gov.scot/images/corporate/pdf/deer-management-on-scotlands-national-forest-estate.pdf\n",
    "document_2_location = \"C:/Users/justi/Documents/GitHub/SmallProjects/documents/deer-management-on-scotlands-national-forest-estate.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(document_location: str) -> str:\n",
    "    extension = os.path.splitext(document_location)[-1].lower()\n",
    "    if extension == '.txt':\n",
    "        with open(document_location) as document:\n",
    "            full_text = document.read()\n",
    "    elif extension == '.pdf':\n",
    "        # https://pypdf.readthedocs.io/en/stable/user/extract-text.html\n",
    "        reader = pypdf.PdfReader(document_location)\n",
    "        number_of_pages = len(reader.pages)\n",
    "        full_text = \"\"\n",
    "        # extract text page by page\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            full_text = full_text + text\n",
    "        # remove linebreaks\n",
    "        full_text = full_text.replace('\\n', ' ')\n",
    "    else:\n",
    "        # try to use textract - warning may be buggy - certainly fails pdfs\n",
    "        # https://textract.readthedocs.io/en/stable/\n",
    "        full_text = textract.process(document_location)\n",
    "    return full_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7d54d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract our documents to plain text\n",
    "text_1 = extract_text(document_1_location)\n",
    "text_2 = extract_text(document_2_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935fd103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_summariser(\n",
    "    text: str,\n",
    "    model: str ='llama3:8b',\n",
    "    input_chunk_tokens: int = 5000, # characters not tokens\n",
    "    chunk_overlap: int = 20,\n",
    "    compression_ratio: float = 5.0,\n",
    "    max_summary_length: int = 5000, # characters not tokens\n",
    "):\n",
    "    \"\"\"\n",
    "    This function takes a text input and recursively summarises it until the summary is less than a specified maximum length.\n",
    "    The function splits the input text into chunks, summarises each chunk, and then concatenates the summaries. \n",
    "    If the concatenated summary is still too long, the function calls itself recursively on the summary to compress further.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to be summarised.\n",
    "    model (str, optional): The model to be used for summarisation. \n",
    "        Defaults to 'llama3:8b'.\n",
    "    input_chunk_tokens (int, optional): The maximum number of characters in each chunk of text to be summarised. \n",
    "        Defaults to 5000.\n",
    "    chunk_overlap (int, optional): The number of characters that consecutive chunks overlap. \n",
    "        Defaults to 20.\n",
    "    compression_ratio (float, optional): The desired compression ratio for the summarisation. \n",
    "        Defaults to 5.0.\n",
    "    max_summary_length (int, optional): The maximum length of the final summary. \n",
    "        Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "    str: The summary of the input text.\n",
    "    \"\"\"\n",
    "    \n",
    "    input_length = len(text)\n",
    "    print(\"Input_text_length: \" + str(len(text)))\n",
    "    # see if we can achieve our desired compressions ratio with a less agressive compression ratio\n",
    "    required_compression = input_length / max_summary_length\n",
    "    if required_compression < compression_ratio:\n",
    "        # this could be a higher compression ration than originally specified, but only for edge cases\n",
    "        # should ensure sufficient compression on the last round of compression to reach a size close to optimal\n",
    "        compression_ratio = required_compression * 1.3 \n",
    "        print(f\"changed compressions ratio to : {compression_ratio}\")\n",
    "    # split the passage based on the token length\n",
    "    text_splitter = langchain.text_splitter.RecursiveCharacterTextSplitter(\n",
    "        chunk_size=input_chunk_tokens, # characters not words\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        # add full stops and commas to the separators to try to get the most sensible splitting \n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \",\", \" \", \"\"],\n",
    "        length_function=len,\n",
    "    #     is_separator_regex=False,\n",
    "    )\n",
    "    text_chunks = text_splitter.split_text(text)\n",
    "    # handle the edge case that we are already down to a chunk smaller than the chunk size\n",
    "    if len(text_chunks) <= 1:\n",
    "        return text\n",
    "    # summarise each chunk in turn\n",
    "    responses = []\n",
    "    for chunk in text_chunks:\n",
    "        word_count = len(chunk.split(\" \"))\n",
    "        desired_word_count = int(word_count//compression_ratio)\n",
    "        print(f\"chunk length: {len(chunk)}, word count: {word_count}, desired word count: {desired_word_count}\")\n",
    "        query = f\"\"\"\n",
    "You are a professional document summariser\n",
    "Please summarise the following text from {word_count} words, down to {desired_word_count} words. \n",
    "Be careful to retain as much of the overall meaning of the text as possible in your summary. \n",
    "Include nothing but the summary in your reply. \n",
    "Do not say how many words it is summarised to. Do not mention that it is a summary.\n",
    "\n",
    "Text:\n",
    "{chunk}\n",
    "\"\"\"\n",
    "        response = ollama.chat(model='llama3:8b', messages=[\n",
    "          {\n",
    "            'role': 'user',\n",
    "            'content': query,\n",
    "          },\n",
    "        ])\n",
    "        response_text = response['message']['content']\n",
    "        responses.append(response_text)\n",
    "    \n",
    "    # concatenate the chunks\n",
    "    summary = \"\\n\".join(responses)\n",
    "    print(\"Summary length in characters: \" + str(len(summary)))\n",
    "    \n",
    "    # check summary length\n",
    "    summary_splitter = langchain.text_splitter.RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_summary_length, # characters not words\n",
    "        chunk_overlap=0,\n",
    "        length_function=len,\n",
    "    #     is_separator_regex=False,\n",
    "    )\n",
    "#     summary_token_count = summary_splitter.count_tokens(summary)\n",
    "    summary_chunks = text_splitter.split_text(summary)\n",
    "    # if summary is short enough return it\n",
    "    if len(summary_chunks) <= 1:\n",
    "        return summary\n",
    "    # otherwise call recursive_summariser on the summary\n",
    "    else:\n",
    "        return recursive_summariser(summary)\n",
    "    \n",
    "def tidy_text(\n",
    "    text: str,\n",
    "    model: str ='llama3:8b',\n",
    "):\n",
    "    \"\"\"\n",
    "    This function takes a text string as input and tries to return a professionally edited version of the text.\n",
    "    It does this using an LLM served via Ollama\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to be edited.\n",
    "    model (str, optional): The model to be used for comparison. \n",
    "        Defaults to 'llama3:8b'.\n",
    "\n",
    "    Returns:\n",
    "    str: The edited text.\n",
    "\n",
    "    Note:\n",
    "    The function uses the 'ollama' chat model for the editing process. The edited text does not mention that it is edited.\n",
    "    \"\"\"\n",
    "        \n",
    "    query = f\"\"\"\n",
    "You are a professional copy editor\n",
    "Please edit the following text ensuring that it maintains a consistent grammatical style throughout.\n",
    "Remove any references to summarisation or word counts.\n",
    "Ensure you preserve all the factual meaning, but remove any incoherent text.\n",
    "Break the text into meaningful paragraphs.\n",
    "Your edited text should be approximately the same number of words as the original.\n",
    "Your edited text must be written in British rather than American English.\n",
    "Include nothing but the edited text in your reply. Do not mention that it is edited.\n",
    "\n",
    "Text:\n",
    "{text}\n",
    "            \"\"\"\n",
    "    edited_summary = ollama.chat(model='llama3:8b', messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': query,\n",
    "      },\n",
    "    ])\n",
    "    return edited_summary['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9ff83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_summary_1 = recursive_summariser(text_1)\n",
    "tidy_summary_1 = tidy_text(raw_summary_1)\n",
    "print(tidy_summary_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cefb485",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_summary_2 = recursive_summariser(text_2)\n",
    "tidy_summary_2 = tidy_text(raw_summary_2)\n",
    "print(tidy_summary_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d8c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_summaries(\n",
    "    summary_1: str,\n",
    "    summary_2: str,\n",
    "    model:str = 'llama3:8b',\n",
    "    max_length:int = 10000,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function compares two summary documents using a specified model served via Ollama\n",
    "    It returns the similarities and differences between them.\n",
    "    The comparison focuses on the content and emphasis of the summaries, rather than their style or phrasing.\n",
    "\n",
    "    Parameters:\n",
    "    summary_1 (str): The first summary to be compared.\n",
    "    summary_2 (str): The second summary to be compared.\n",
    "    model (str, optional): The model to be used for comparison. \n",
    "        Defaults to 'llama3:8b'.\n",
    "    max_length (int, optional): The maximum length of the query in characters. \n",
    "        Defaults to 10000.\n",
    "\n",
    "    Returns:\n",
    "    str: The similarities and differences between the two summaries.\n",
    "\n",
    "    Raises:\n",
    "    Warning: If the length of the query exceeds the max_length in characters, a warning message is printed.\n",
    "    \"\"\"\n",
    "    \n",
    "    query = f\"\"\"\n",
    "Please describe the similarities and differences between the following two passages. \n",
    "Focus on similarites and differences of content and emphasis, rather than style or phrasing.\n",
    "Provide your answers as two numbered lists labelled SIMILARITIES and DIFFERENCES.\n",
    "\n",
    "Passage 1:\n",
    "{summary_1}\n",
    "\n",
    "passage 2:\n",
    "{summary_2}\n",
    "\"\"\"\n",
    "    if len(query) > max_length:\n",
    "        print(\"Warning query may exceed context window\")\n",
    "    response = ollama.chat(model=model, messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': query,\n",
    "      },\n",
    "    ])\n",
    "    answer = response['message']['content']\n",
    "    return(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50db1f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = compare_summaries(tidy_summary_1, tidy_summary_2)\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b3d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_strategies(\n",
    "    summary_1: str,\n",
    "    summary_2: str,\n",
    "    model:str = 'llama3:8b',\n",
    "    max_length:int = 10000,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function compares two summary documents using a specified model served via Ollama\n",
    "    It returns the similarities and differences between them.\n",
    "    The comparison focuses on thestrategy outlined in the summaries, rather than their style or phrasing.\n",
    "\n",
    "    Parameters:\n",
    "    summary_1 (str): The first summary to be compared.\n",
    "    summary_2 (str): The second summary to be compared.\n",
    "    model (str, optional): The model to be used for comparison. Defaults to 'llama3:8b'.\n",
    "    max_length (int, optional): The maximum length of the query in characters. Defaults to 10000.\n",
    "\n",
    "    Returns:\n",
    "    str: The similarities and differences between the two summaries.\n",
    "\n",
    "    Raises:\n",
    "    Warning: If the length of the query exceeds the max_length in characters, a warning message is printed.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "You are an enviromental strategy expert.\n",
    "Please describe the similarities and differences in the strategies outlined by the following passages. \n",
    "Focus on similarites and differences of strategy, rather than style, tone or phrasing.\n",
    "Provide your answers as two numbered lists labelled SIMILARITIES and DIFFERENCES.\n",
    "\n",
    "Passage 1:\n",
    "{summary_1}\n",
    "\n",
    "passage 2:\n",
    "{summary_2}\n",
    "\"\"\"\n",
    "    if len(query) > max_length:\n",
    "        print(\"Warning query may exceed context window\")\n",
    "    response = ollama.chat(model=model, messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': query,\n",
    "      },\n",
    "    ])\n",
    "    answer = response['message']['content']\n",
    "    return(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4496f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_comparison = compare_strategies(tidy_summary_1, tidy_summary_2)\n",
    "print(strategy_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c4eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

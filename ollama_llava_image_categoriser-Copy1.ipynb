{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88fe9c66",
   "metadata": {},
   "source": [
    " # Image categoriser\n",
    " \n",
    " A proof of concept for a zero-shot photograph categoriser using OpenAI's gpt-4-vision-preview VLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27756a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import at the library level to avoid function name confusion between libraries for simplicity\n",
    "import datetime\n",
    "import fnmatch\n",
    "import functools\n",
    "import ollama\n",
    "import os\n",
    "import pandas as pd\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9141537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our requirements for classification\n",
    "folder_to_classify = \"D:/pictures\"\n",
    "style_categories = [\"Photo\", \"Art\", \"Diagram\", \"Other\"]\n",
    "subject_categories = [\"Landscape\", \"Urban\", \"Portrait\", \"Pets\", \"Wildlife\", \"Other\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f6fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc53c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_files_generator(directory: str) -> typing.Generator[str, None , None]:\n",
    "    \"\"\"\n",
    "    Recursively finds all image files in the specified directory and its subdirectories.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The path to the directory to search in.\n",
    "\n",
    "    Returns:\n",
    "        Generator[str]: paths to image files found yielded one at a time\n",
    "    \"\"\"\n",
    "    image_extensions = ['*.jpg', '*.jpeg', '*.png']#, '*.gif', '*.bmp', '*.tiff']\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for extension in image_extensions:\n",
    "            for filename in fnmatch.filter(files, extension):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                yield file_path.replace(\"\\\\\", \"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873bd403",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = list(image_files_generator(directory = folder_to_classify))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a4431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_response(response: str) -> tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Decodes the response from a request.\n",
    "\n",
    "    This function takes a response from a request, extracts the content, \n",
    "    splits it into style, category, and description, and returns these as a tuple. \n",
    "    If the content does not contain all three elements, it makes assumptions \n",
    "    about the missing elements.\n",
    "\n",
    "    Args:\n",
    "        response (str): The response from a request to the VLM model\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, str, str]: A tuple containing the style, category, and description.\n",
    "    \"\"\"\n",
    "    contents = response.split('|')\n",
    "    style = contents[0]\n",
    "    try:\n",
    "        category = contents[1]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        description = contents[2]\n",
    "    except IndexError:\n",
    "        # if we are missing indices we cannot assume anything is correct so place what we do have in the description\n",
    "        description = contents[0]\n",
    "        style = \"unknown\"\n",
    "        category = \"unknown\"\n",
    "    return style, category, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b47485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorise(\n",
    "    image_path: str,\n",
    "    style_categories: list[str],\n",
    "    subject_categories: list[str]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Categorize an image based on style, subject, and description.\n",
    "\n",
    "    This function takes an image file path, permissible style categories,\n",
    "    and permissible subject categories. It sends the image to a local Ollama endpoint running LLaVA for analysis and\n",
    "    returns a pipe-separated string containing the most appropriate style, subject, and description.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): The file path to the image.\n",
    "        param style_categories (List[str]): List of permissible style categories.\n",
    "        param subject_categories (List[str]):  List of permissible subject categories.\n",
    "    \n",
    "    Returns: \n",
    "        Tuple[str, str, str]: A pipe-separated string in the format \"Style|Subject|Description\".\n",
    "    \"\"\"\n",
    "\n",
    "    # compile the query to go with the image\n",
    "    text = f\"\"\"Please categorise this image.\n",
    "To do this you will return a pipe separated string of the form: \n",
    "Style|Subject|Description \n",
    "Permissible styles are: {\", \".join(style_categories)} . Pick only the most appropriate one.\n",
    "Permissible subjects are: {\", \".join(subject_categories)} . Pick only the most appropriate one.\n",
    "The description should be no more than 30 words long and should describe the picture as accurately as possible\n",
    "Return no other detail other than the 3 pipe separated items so for instance if the style was {style_categories[0]}\n",
    "and the subject was {subject_categories[0]} and the desciption was 'A small apple in a green bowl' you would return:\n",
    "{style_categories[0]}|{subject_categories[0]}|A small apple in a green bowl\n",
    "\"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=\"llava\",\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': text,\n",
    "                'images': [image_path]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    style, category, description = decode_response(response['message']['content'])\n",
    "\n",
    "    return style, category, description \n",
    "\n",
    "partial_categorise = functools.partial(\n",
    "    categorise,\n",
    "    style_categories = style_categories,\n",
    "    subject_categories = subject_categories,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2809b8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorise_image_files(image_files:list[str]) -> typing.Generator[dict[str], None , None]:\n",
    "    \"\"\"\n",
    "    Categorises a list of image files.\n",
    "\n",
    "    This function takes a list of image file paths, categorises each image, \n",
    "    and returns a list of dictionaries. Each dictionary contains the file path \n",
    "    and its corresponding categories.\n",
    "\n",
    "    Args:\n",
    "        image_files (List[str]): A list of image file paths.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of dictionaries. Each dictionary contains \n",
    "        'file' (the file path) and 'categories' (the categories of the image).\n",
    "    \"\"\"\n",
    "    for image_file in image_files:\n",
    "        style, category, description = partial_categorise(image_file)\n",
    "        category_dict = {\n",
    "            'file': image_file,\n",
    "            'style': style,\n",
    "            'category': category,\n",
    "            'description': description,\n",
    "        }\n",
    "        yield category_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e8f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorised_pictures = list(categorise_image_files(image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db9e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pictures = pd.DataFrame(categorised_pictures)\n",
    "display(df_pictures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9730e5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results out to durable storage\n",
    "df_pictures.to_excel(folder_to_classify + \"/llava_picture_classification.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ab40c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.datetime.now()\n",
    "total_time = end_time - start_time\n",
    "print(total_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

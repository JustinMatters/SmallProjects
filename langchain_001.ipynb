{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9667125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# installs with pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#installs needed to use langchain\n",
    "# pip install huggingface_hub\n",
    "# pip install openai\n",
    "# pip install langchain\n",
    "# from langchain.llms import OpenAI # deprecated?\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628016fc",
   "metadata": {},
   "source": [
    "# Basic OpenAI usage with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0319666a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ChatOpenAI in module langchain.chat_models.openai:\n",
      "\n",
      "class ChatOpenAI(langchain.chat_models.base.BaseChatModel)\n",
      " |  ChatOpenAI(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, client: Any = None, model: str = 'gpt-3.5-turbo', temperature: float = 0.7, model_kwargs: Dict[str, Any] = None, openai_api_key: Optional[str] = None, openai_api_base: Optional[str] = None, openai_organization: Optional[str] = None, openai_proxy: Optional[str] = None, request_timeout: Union[float, Tuple[float, float], NoneType] = None, max_retries: int = 6, streaming: bool = False, n: int = 1, max_tokens: Optional[int] = None) -> None\n",
      " |  \n",
      " |  Wrapper around OpenAI Chat large language models.\n",
      " |  \n",
      " |  To use, you should have the ``openai`` python package installed, and the\n",
      " |  environment variable ``OPENAI_API_KEY`` set with your API key.\n",
      " |  \n",
      " |  Any parameters that are valid to be passed to the openai.create call can be passed\n",
      " |  in, even if not explicitly saved on this class.\n",
      " |  \n",
      " |  Example:\n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          from langchain.chat_models import ChatOpenAI\n",
      " |          openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ChatOpenAI\n",
      " |      langchain.chat_models.base.BaseChatModel\n",
      " |      langchain.base_language.BaseLanguageModel\n",
      " |      langchain.load.serializable.Serializable\n",
      " |      pydantic.main.BaseModel\n",
      " |      pydantic.utils.Representation\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  completion_with_retry(self, **kwargs: 'Any') -> 'Any'\n",
      " |      Use tenacity to retry the completion call.\n",
      " |  \n",
      " |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      " |      Calculate num tokens for gpt-3.5-turbo and gpt-4 with tiktoken package.\n",
      " |      \n",
      " |      Official documentation: https://github.com/openai/openai-cookbook/blob/\n",
      " |      main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n",
      " |  \n",
      " |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      " |      Get the tokens present in the text with tiktoken package.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  build_extra(values: 'Dict[str, Any]') -> 'Dict[str, Any]' from pydantic.main.ModelMetaclass\n",
      " |      Build extra kwargs from additional params that were passed in.\n",
      " |  \n",
      " |  validate_environment(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      " |      Validate that api key and python package exists in environment.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  lc_secrets\n",
      " |      Return a map of constructor argument names to secret ids.\n",
      " |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      " |  \n",
      " |  lc_serializable\n",
      " |      Return whether or not the class is serializable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Config = <class 'langchain.chat_models.openai.ChatOpenAI.Config'>\n",
      " |      Configuration for this pydantic object.\n",
      " |  \n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'client': 'Any', 'max_retries': 'int', 'max_tokens'...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __config__ = <class 'langchain.chat_models.openai.Config'>\n",
      " |  \n",
      " |  __custom_root_type__ = False\n",
      " |  \n",
      " |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'ta...\n",
      " |  \n",
      " |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __include_fields__ = None\n",
      " |  \n",
      " |  __post_root_validators__ = [(False, <function BaseChatModel.raise_depr...\n",
      " |  \n",
      " |  __pre_root_validators__ = [<function ChatOpenAI.build_extra>]\n",
      " |  \n",
      " |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      " |  \n",
      " |  __schema_cache__ = {}\n",
      " |  \n",
      " |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...t =...\n",
      " |  \n",
      " |  __validators__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain.chat_models.base.BaseChatModel:\n",
      " |  \n",
      " |  __call__(self, messages: List[langchain.schema.BaseMessage], stop: Optional[List[str]] = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, **kwargs: Any) -> langchain.schema.BaseMessage\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  async agenerate(self, messages: List[List[langchain.schema.BaseMessage]], stop: Optional[List[str]] = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, *, tags: Optional[List[str]] = None, **kwargs: Any) -> langchain.schema.LLMResult\n",
      " |      Top Level call\n",
      " |  \n",
      " |  async agenerate_prompt(self, prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, **kwargs: Any) -> langchain.schema.LLMResult\n",
      " |      Take in a list of prompt values and return an LLMResult.\n",
      " |  \n",
      " |  async apredict(self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any) -> str\n",
      " |      Predict text from text.\n",
      " |  \n",
      " |  async apredict_messages(self, messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None, **kwargs: Any) -> langchain.schema.BaseMessage\n",
      " |      Predict message from messages.\n",
      " |  \n",
      " |  call_as_llm(self, message: str, stop: Optional[List[str]] = None, **kwargs: Any) -> str\n",
      " |  \n",
      " |  dict(self, **kwargs: Any) -> Dict\n",
      " |      Return a dictionary of the LLM.\n",
      " |  \n",
      " |  generate(self, messages: List[List[langchain.schema.BaseMessage]], stop: Optional[List[str]] = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, *, tags: Optional[List[str]] = None, **kwargs: Any) -> langchain.schema.LLMResult\n",
      " |      Top Level call\n",
      " |  \n",
      " |  generate_prompt(self, prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, **kwargs: Any) -> langchain.schema.LLMResult\n",
      " |      Take in a list of prompt values and return an LLMResult.\n",
      " |  \n",
      " |  predict(self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any) -> str\n",
      " |      Predict text from text.\n",
      " |  \n",
      " |  predict_messages(self, messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None, **kwargs: Any) -> langchain.schema.BaseMessage\n",
      " |      Predict message from messages.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain.chat_models.base.BaseChatModel:\n",
      " |  \n",
      " |  raise_deprecation(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      " |      Raise deprecation warning if callback_manager is used.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain.base_language.BaseLanguageModel:\n",
      " |  \n",
      " |  get_num_tokens(self, text: 'str') -> 'int'\n",
      " |      Get the number of tokens present in the text.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain.base_language.BaseLanguageModel:\n",
      " |  \n",
      " |  all_required_field_names() -> 'Set' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain.load.serializable.Serializable:\n",
      " |  \n",
      " |  __init__(self, **kwargs: Any) -> None\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |      \n",
      " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      " |  \n",
      " |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      " |  \n",
      " |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      " |  \n",
      " |  lc_attributes\n",
      " |      Return a list of attribute names that should be included in the\n",
      " |      serialized kwargs. These attributes must be accepted by the\n",
      " |      constructor.\n",
      " |  \n",
      " |  lc_namespace\n",
      " |      Return the namespace of the langchain object.\n",
      " |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |  \n",
      " |  __getstate__(self) -> 'DictAny'\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |  \n",
      " |  __repr_args__(self) -> 'ReprArgs'\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |  \n",
      " |  __setstate__(self, state: 'DictAny') -> None\n",
      " |  \n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |      \n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |  \n",
      " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |      \n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |  \n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |  \n",
      " |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |  \n",
      " |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.utils.Representation:\n",
      " |  \n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |  \n",
      " |  __repr__(self) -> 'unicode'\n",
      " |  \n",
      " |  __repr_name__(self) -> 'unicode'\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Get fields for Rich library\n",
      " |  \n",
      " |  __str__(self) -> 'unicode'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ChatOpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f06f257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43422e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that if openai_api_key is omitted, OpenAI will attempt to get it direct from environment variables\n",
    "gpt_model = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-0613\", \n",
    "    temperature=0.7, \n",
    "    max_tokens=300, \n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5676647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gpt_model.predict(\"Give a summary of Von Clauswitz's book 'On War' in less than 100 words\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f24865",
   "metadata": {},
   "source": [
    "# Basic Hugging Face usage with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0841211",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACEHUB_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ff1c788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class HuggingFaceHub in module langchain.llms.huggingface_hub:\n",
      "\n",
      "class HuggingFaceHub(langchain.llms.base.LLM)\n",
      " |  HuggingFaceHub(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, client: Any = None, repo_id: str = 'gpt2', task: Optional[str] = None, model_kwargs: Optional[dict] = None, huggingfacehub_api_token: Optional[str] = None) -> None\n",
      " |  \n",
      " |  Wrapper around HuggingFaceHub  models.\n",
      " |  \n",
      " |  To use, you should have the ``huggingface_hub`` python package installed, and the\n",
      " |  environment variable ``HUGGINGFACEHUB_API_TOKEN`` set with your API token, or pass\n",
      " |  it as a named parameter to the constructor.\n",
      " |  \n",
      " |  Only supports `text-generation`, `text2text-generation` and `summarization` for now.\n",
      " |  \n",
      " |  Example:\n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          from langchain.llms import HuggingFaceHub\n",
      " |          hf = HuggingFaceHub(repo_id=\"gpt2\", huggingfacehub_api_token=\"my-api-key\")\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      HuggingFaceHub\n",
      " |      langchain.llms.base.LLM\n",
      " |      langchain.llms.base.BaseLLM\n",
      " |      langchain.base_language.BaseLanguageModel\n",
      " |      langchain.load.serializable.Serializable\n",
      " |      pydantic.main.BaseModel\n",
      " |      pydantic.utils.Representation\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      " |      Validate that api key and python package exists in environment.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Config = <class 'langchain.llms.huggingface_hub.HuggingFaceHub.Config'...\n",
      " |      Configuration for this pydantic object.\n",
      " |  \n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'client': typing.Any, 'huggingfacehub_api_token': t...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __config__ = <class 'langchain.llms.huggingface_hub.Config'>\n",
      " |  \n",
      " |  __custom_root_type__ = False\n",
      " |  \n",
      " |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'ta...\n",
      " |  \n",
      " |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __include_fields__ = None\n",
      " |  \n",
      " |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      " |  \n",
      " |  __pre_root_validators__ = []\n",
      " |  \n",
      " |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      " |  \n",
      " |  __schema_cache__ = {}\n",
      " |  \n",
      " |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...fac...\n",
      " |  \n",
      " |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      " |  \n",
      " |  __call__(self, prompt: str, stop: Optional[List[str]] = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, **kwargs: Any) -> str\n",
      " |      Check Cache and run the LLM on the given prompt and input.\n",
      " |  \n",
      " |  __str__(self) -> str\n",
      " |      Get a string representation of the object for printing.\n",
      " |  \n",
      " |  async agenerate(self, prompts: List[str], stop: Optional[List[str]] = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, *, tags: Optional[List[str]] = None, **kwargs: Any) -> langchain.schema.LLMResult\n",
      " |      Run the LLM on the given prompt and input.\n",
      " |  \n",
      " |  async agenerate_prompt(self, prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, **kwargs: Any) -> langchain.schema.LLMResult\n",
      " |      Take in a list of prompt values and return an LLMResult.\n",
      " |  \n",
      " |  async apredict(self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any) -> str\n",
      " |      Predict text from text.\n",
      " |  \n",
      " |  async apredict_messages(self, messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None, **kwargs: Any) -> langchain.schema.BaseMessage\n",
      " |      Predict message from messages.\n",
      " |  \n",
      " |  dict(self, **kwargs: Any) -> Dict\n",
      " |      Return a dictionary of the LLM.\n",
      " |  \n",
      " |  generate(self, prompts: List[str], stop: Optional[List[str]] = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, *, tags: Optional[List[str]] = None, **kwargs: Any) -> langchain.schema.LLMResult\n",
      " |      Run the LLM on the given prompt and input.\n",
      " |  \n",
      " |  generate_prompt(self, prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, **kwargs: Any) -> langchain.schema.LLMResult\n",
      " |      Take in a list of prompt values and return an LLMResult.\n",
      " |  \n",
      " |  predict(self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any) -> str\n",
      " |      Predict text from text.\n",
      " |  \n",
      " |  predict_messages(self, messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None, **kwargs: Any) -> langchain.schema.BaseMessage\n",
      " |      Predict message from messages.\n",
      " |  \n",
      " |  save(self, file_path: Union[pathlib.Path, str]) -> None\n",
      " |      Save the LLM.\n",
      " |      \n",
      " |      Args:\n",
      " |          file_path: Path to file to save the LLM to.\n",
      " |      \n",
      " |      Example:\n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          llm.save(file_path=\"path/llm.yaml\")\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      " |  \n",
      " |  raise_deprecation(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      " |      Raise deprecation warning if callback_manager is used.\n",
      " |  \n",
      " |  set_verbose(verbose: Optional[bool]) -> bool from pydantic.main.ModelMetaclass\n",
      " |      If verbose is None, set it.\n",
      " |      \n",
      " |      This allows users to pass in None as verbose to access the global setting.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain.base_language.BaseLanguageModel:\n",
      " |  \n",
      " |  get_num_tokens(self, text: 'str') -> 'int'\n",
      " |      Get the number of tokens present in the text.\n",
      " |  \n",
      " |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      " |      Get the number of tokens in the message.\n",
      " |  \n",
      " |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      " |      Get the token present in the text.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain.base_language.BaseLanguageModel:\n",
      " |  \n",
      " |  all_required_field_names() -> 'Set' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain.load.serializable.Serializable:\n",
      " |  \n",
      " |  __init__(self, **kwargs: Any) -> None\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |      \n",
      " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      " |  \n",
      " |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      " |  \n",
      " |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      " |  \n",
      " |  lc_attributes\n",
      " |      Return a list of attribute names that should be included in the\n",
      " |      serialized kwargs. These attributes must be accepted by the\n",
      " |      constructor.\n",
      " |  \n",
      " |  lc_namespace\n",
      " |      Return the namespace of the langchain object.\n",
      " |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      " |  \n",
      " |  lc_secrets\n",
      " |      Return a map of constructor argument names to secret ids.\n",
      " |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      " |  \n",
      " |  lc_serializable\n",
      " |      Return whether or not the class is serializable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |  \n",
      " |  __getstate__(self) -> 'DictAny'\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |  \n",
      " |  __repr_args__(self) -> 'ReprArgs'\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |  \n",
      " |  __setstate__(self, state: 'DictAny') -> None\n",
      " |  \n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |      \n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |  \n",
      " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |      \n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |  \n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |  \n",
      " |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |  \n",
      " |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.utils.Representation:\n",
      " |  \n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |  \n",
      " |  __repr__(self) -> 'unicode'\n",
      " |  \n",
      " |  __repr_name__(self) -> 'unicode'\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Get fields for Rich library\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(HuggingFaceHub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84c6e4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\justi\\Anaconda3\\envs\\langchain\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "hf_model = HuggingFaceHub(\n",
    "    repo_id='google/flan-t5-base',\n",
    "    model_kwargs={'temperature':0.1},\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6f3119e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sydney\n"
     ]
    }
   ],
   "source": [
    "print(hf_model(\"what is the capital of Finland?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96b0cb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helsinki\n"
     ]
    }
   ],
   "source": [
    "print(hf_model(\"what would wikipedia say is the capital of Finland?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f66128",
   "metadata": {},
   "source": [
    "# Demonstrate a langchain prompt\n",
    "\n",
    "Prompts can consit of several elements\n",
    "- Context\n",
    "- Question / Task\n",
    "- Examples\n",
    "- output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41fc50f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a naval historian with experience in the Napoleonic period.\n",
    "Return a list of ten relevant ship names for ships which existed in this period for {country}.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbb9ec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"country\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51a2c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that if openai_api_key is omitted, OpenAI will attempt to get it direct from environment variables\n",
    "gpt_model_2 = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-0613\", \n",
    "    temperature=0.7, \n",
    "    max_tokens=300, \n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf18e042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. HMS Victory - One of the most famous British warships of the Napoleonic era, it served as Admiral Nelson's flagship at the Battle of Trafalgar in 1805.\n",
      "2. HMS Royal Sovereign - A first-rate ship of the line, it played a crucial role in the Battle of Cape St Vincent in 1797.\n",
      "3. HMS Bellerophon - This third-rate ship of the line gained fame for being the vessel that Napoleon Bonaparte surrendered on after the Battle of Waterloo in 1815.\n",
      "4. HMS Agamemnon - A 64-gun third-rate ship, it was commanded by Nelson during his early career and participated in several major battles.\n",
      "5. HMS Temeraire - Known as the \"Fighting Temeraire,\" this second-rate ship played a pivotal role in the Battle of Trafalgar and was immortalized in a famous painting by J.M.W. Turner.\n",
      "6. HMS Dreadnought - A first-rate ship of the line, it was launched in 1801 and served during the Napoleonic Wars, including Trafalgar.\n",
      "7. HMS Superb - A 74-gun third-rate ship, it was involved in numerous engagements against French and Spanish fleets.\n",
      "8. HMS Indefatigable - A powerful 44-gun frigate, it was renowned for its speed and endurance, participating in several successful naval actions.\n",
      "9. HMS Defiance - A 74-gun third-rate ship, it\n"
     ]
    }
   ],
   "source": [
    "# we can use the template direct\n",
    "print(gpt_model_2.predict(text=prompt.format(country=\"Britain\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5d1ce9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Le Vengeur - A 74-gun ship of the line\n",
      "2. Le Bucentaure - A 80-gun ship of the line\n",
      "3. L'Orient - A 120-gun ship of the line\n",
      "4. Le Redoutable - A 74-gun ship of the line\n",
      "5. Le Formidable - A 80-gun ship of the line\n",
      "6. Le Neptune - A 74-gun ship of the line\n",
      "7. L'Aigle - A 74-gun ship of the line\n",
      "8. Le Fougueux - A 74-gun ship of the line\n",
      "9. Le Téméraire - A 74-gun ship of the line\n",
      "10. L'Intrépide - A 74-gun ship of the line\n"
     ]
    }
   ],
   "source": [
    "# or alternatively we can sart to chain out work together\n",
    "chain = LLMChain(\n",
    "    llm=gpt_model_2,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "print(chain.run(\"France\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bf6328",
   "metadata": {},
   "source": [
    "# Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b294974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhyme_examples = [\n",
    "    {'word': 'sun', 'rhyme': 'bun'},\n",
    "    {'word': 'float', 'rhyme': 'goat'},\n",
    "    {'word': 'remember', 'rhyme': 'november'},\n",
    "]\n",
    "\n",
    "rhyme_template = \"\"\"\n",
    "Word: {word}\n",
    "Rhyme: {rhyme}\n",
    "\"\"\"\n",
    "\n",
    "rhyme_prompt = PromptTemplate(\n",
    "    input_variables=['word', 'rhyme'],\n",
    "    template=rhyme_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a922c48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWord: word\\nRhyme: bird\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can format for inputs as before but we need to use the fewshot prompt to build the rhyme_examples into a prompt\n",
    "rhyme_prompt.format(word='word', rhyme='bird')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b033e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    # These are the examples we want to insert into the prompt.\n",
    "    examples=rhyme_examples,\n",
    "    # This is how we want to format the examples when we insert them into the prompt.\n",
    "    example_prompt=rhyme_prompt,\n",
    "    # The prefix is some text that goes before the examples in the prompt.\n",
    "    # Usually, this consists of intructions.\n",
    "    prefix=\"Give a rhyming word for each input\",\n",
    "    # The suffix is some text that goes after the examples in the prompt.\n",
    "    # Usually, this is where the user input will go\n",
    "    suffix=\"\"\"Word: {input}\\nRhyme:\\n\n",
    "    \"\"\",\n",
    "    # The input variables are the variables that the overall prompt expects.\n",
    "    input_variables=[\"input\"],\n",
    "    # The example_separator is the string we will use to join the prefix, examples, and suffix together with.\n",
    "    example_separator=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14fb13e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give a rhyming word for each input\n",
      "\n",
      "Word: sun\n",
      "Rhyme: bun\n",
      "\n",
      "\n",
      "Word: float\n",
      "Rhyme: goat\n",
      "\n",
      "\n",
      "Word: remember\n",
      "Rhyme: november\n",
      "\n",
      "Word: round\n",
      "Rhyme:\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(few_shot_prompt.format(input=\"round\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6aaf146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_2 = HuggingFaceHub(\n",
    "    repo_id='google/flan-t5-base',\n",
    "    model_kwargs={'temperature':0.1},\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "479705f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round\n"
     ]
    }
   ],
   "source": [
    "# google flan tends to be terrible\n",
    "print(hf_model_2(few_shot_prompt.format(input=\"round\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efaf8b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cough\n"
     ]
    }
   ],
   "source": [
    "# google flan tends to be terrible\n",
    "print(hf_model_2(few_shot_prompt.format(input=\"cough\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a33b32ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model_3 = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-0613\", \n",
    "    temperature=0.7, \n",
    "    max_tokens=300, \n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28d52ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bound\n"
     ]
    }
   ],
   "source": [
    "# chatGPT is rather better\n",
    "print(gpt_model_3.predict(few_shot_prompt.format(input=\"round\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef86494d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cough\n"
     ]
    }
   ],
   "source": [
    "# chatGPT is rather better\n",
    "print(gpt_model_3.predict(few_shot_prompt.format(input=\"enough\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94829100",
   "metadata": {},
   "source": [
    "# Tools\n",
    "these include\n",
    "- Python REPL\n",
    "- Search engine APIs (there is a DuckDuckGo API)\n",
    "- Wolfram API\n",
    "- SQL\n",
    "- Requests chain to get a response from the web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03857799",
   "metadata": {},
   "source": [
    "# Chains\n",
    "- join LLMs, prompts and tools\n",
    "- pass outputs as next inputs\n",
    "- similar to ML pipelines\n",
    "\n",
    "## Types\n",
    "- Generic chains\n",
    "- Utility chains\n",
    "- Asynchronous chains\n",
    "\n",
    "- Transformation chain - applies code like transformation (eg regex)\n",
    "- Chains can be made of other chains\n",
    "\n",
    "CAn run python code, SQL, query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "349e9507",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_rows = [0,1,2,3,4]\n",
    "#ast_rows[-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecc1dcfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba94796b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_rows[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e19164e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_rows\n",
      "[[0.57877107]]\n",
      "new_rows\n",
      "[[ 1.14146815]\n",
      " [ 0.44200552]\n",
      " [ 0.37335953]\n",
      " [-0.69548122]\n",
      " [-0.9493953 ]]\n",
      "last_rows\n",
      "[[ 1.14146815]\n",
      " [ 0.44200552]\n",
      " [ 0.37335953]\n",
      " [-0.69548122]\n",
      " [-0.9493953 ]]\n",
      "new_rows\n",
      "[[-1.04282169]\n",
      " [-1.03977773]\n",
      " [-1.342834  ]\n",
      " [-1.43826013]\n",
      " [-1.0251471 ]]\n",
      "last_rows\n",
      "[[-1.04282169]\n",
      " [-1.03977773]\n",
      " [-1.342834  ]\n",
      " [-1.43826013]\n",
      " [-1.0251471 ]]\n",
      "new_rows\n",
      "[[-0.31778677]\n",
      " [-0.80532854]\n",
      " [-1.59685401]\n",
      " [-3.19442716]\n",
      " [-3.41181018]]\n",
      "last_rows\n",
      "[[-0.31778677]\n",
      " [-0.80532854]\n",
      " [-1.59685401]\n",
      " [-3.19442716]\n",
      " [-3.41181018]]\n",
      "new_rows\n",
      "[[-2.10651861]\n",
      " [-0.85957326]\n",
      " [ 0.09785644]\n",
      " [-1.99628729]\n",
      " [-1.50590078]]\n",
      "last_rows\n",
      "[[-2.10651861]\n",
      " [-0.85957326]\n",
      " [ 0.09785644]\n",
      " [-1.99628729]\n",
      " [-1.50590078]]\n",
      "new_rows\n",
      "[[-2.74877551]\n",
      " [-3.29001392]\n",
      " [-2.2810073 ]\n",
      " [-3.78685056]\n",
      " [-4.17211442]]\n",
      "last_rows\n",
      "[[-2.74877551]\n",
      " [-3.29001392]\n",
      " [-2.2810073 ]\n",
      " [-3.78685056]\n",
      " [-4.17211442]]\n",
      "new_rows\n",
      "[[-4.58915112]\n",
      " [-5.33769398]\n",
      " [-5.79238979]\n",
      " [-5.84635922]\n",
      " [-4.07620791]]\n",
      "last_rows\n",
      "[[-4.58915112]\n",
      " [-5.33769398]\n",
      " [-5.79238979]\n",
      " [-5.84635922]\n",
      " [-4.07620791]]\n",
      "new_rows\n",
      "[[-4.2625058 ]\n",
      " [-2.56757012]\n",
      " [-3.27873569]\n",
      " [-3.00745424]\n",
      " [-3.2414998 ]]\n",
      "last_rows\n",
      "[[-4.2625058 ]\n",
      " [-2.56757012]\n",
      " [-3.27873569]\n",
      " [-3.00745424]\n",
      " [-3.2414998 ]]\n",
      "new_rows\n",
      "[[-2.02843912]\n",
      " [-0.51017634]\n",
      " [ 0.21065032]\n",
      " [ 0.08370119]\n",
      " [ 0.38862418]]\n",
      "last_rows\n",
      "[[-2.02843912]\n",
      " [-0.51017634]\n",
      " [ 0.21065032]\n",
      " [ 0.08370119]\n",
      " [ 0.38862418]]\n",
      "new_rows\n",
      "[[0.41726999]\n",
      " [1.69700986]\n",
      " [2.5211624 ]\n",
      " [1.90304646]\n",
      " [1.01693115]]\n",
      "last_rows\n",
      "[[0.41726999]\n",
      " [1.69700986]\n",
      " [2.5211624 ]\n",
      " [1.90304646]\n",
      " [1.01693115]]\n",
      "new_rows\n",
      "[[2.39642265]\n",
      " [3.21911545]\n",
      " [3.03969393]\n",
      " [1.72786492]\n",
      " [1.33230697]]\n",
      "last_rows\n",
      "[[2.39642265]\n",
      " [3.21911545]\n",
      " [3.03969393]\n",
      " [1.72786492]\n",
      " [1.33230697]]\n",
      "new_rows\n",
      "[[ 1.30999622]\n",
      " [ 1.67316337]\n",
      " [ 1.02655283]\n",
      " [ 0.15193313]\n",
      " [-0.92562267]]\n",
      "last_rows\n",
      "[[ 1.30999622]\n",
      " [ 1.67316337]\n",
      " [ 1.02655283]\n",
      " [ 0.15193313]\n",
      " [-0.92562267]]\n",
      "new_rows\n",
      "[[ 0.67842599]\n",
      " [-0.60184781]\n",
      " [ 0.16163911]\n",
      " [-1.26940495]\n",
      " [-2.98289387]]\n",
      "last_rows\n",
      "[[ 0.67842599]\n",
      " [-0.60184781]\n",
      " [ 0.16163911]\n",
      " [-1.26940495]\n",
      " [-2.98289387]]\n",
      "new_rows\n",
      "[[-2.84130911]\n",
      " [-3.03729286]\n",
      " [-3.70720359]\n",
      " [-3.14895652]\n",
      " [-3.78239422]]\n",
      "last_rows\n",
      "[[-2.84130911]\n",
      " [-3.03729286]\n",
      " [-3.70720359]\n",
      " [-3.14895652]\n",
      " [-3.78239422]]\n",
      "new_rows\n",
      "[[-4.57317364]\n",
      " [-4.92976806]\n",
      " [-6.48462406]\n",
      " [-7.95646398]\n",
      " [-7.59634184]]\n",
      "last_rows\n",
      "[[-4.57317364]\n",
      " [-4.92976806]\n",
      " [-6.48462406]\n",
      " [-7.95646398]\n",
      " [-7.59634184]]\n",
      "new_rows\n",
      "[[-7.38527438]\n",
      " [-8.72466536]\n",
      " [-8.59551304]\n",
      " [-8.54389604]\n",
      " [-8.69177655]]\n",
      "last_rows\n",
      "[[-7.38527438]\n",
      " [-8.72466536]\n",
      " [-8.59551304]\n",
      " [-8.54389604]\n",
      " [-8.69177655]]\n",
      "new_rows\n",
      "[[-9.69709401]\n",
      " [-9.46292179]\n",
      " [-8.1973016 ]\n",
      " [-8.22255124]\n",
      " [-9.9405669 ]]\n",
      "last_rows\n",
      "[[-9.69709401]\n",
      " [-9.46292179]\n",
      " [-8.1973016 ]\n",
      " [-8.22255124]\n",
      " [-9.9405669 ]]\n",
      "new_rows\n",
      "[[-12.01492651]\n",
      " [-11.45484174]\n",
      " [-11.52992259]\n",
      " [-10.8639617 ]\n",
      " [-10.13670072]]\n",
      "last_rows\n",
      "[[-12.01492651]\n",
      " [-11.45484174]\n",
      " [-11.52992259]\n",
      " [-10.8639617 ]\n",
      " [-10.13670072]]\n",
      "new_rows\n",
      "[[ -9.29040365]\n",
      " [ -9.16604766]\n",
      " [-10.03648259]\n",
      " [ -8.15962849]\n",
      " [ -6.74914044]]\n",
      "last_rows\n",
      "[[ -9.29040365]\n",
      " [ -9.16604766]\n",
      " [-10.03648259]\n",
      " [ -8.15962849]\n",
      " [ -6.74914044]]\n",
      "new_rows\n",
      "[[-6.19446517]\n",
      " [-6.19973323]\n",
      " [-6.19189794]\n",
      " [-5.71206203]\n",
      " [-6.78974164]]\n",
      "last_rows\n",
      "[[-6.19446517]\n",
      " [-6.19973323]\n",
      " [-6.19189794]\n",
      " [-5.71206203]\n",
      " [-6.78974164]]\n",
      "new_rows\n",
      "[[-7.8815317 ]\n",
      " [-8.25390709]\n",
      " [-9.13786165]\n",
      " [-7.84285434]\n",
      " [-9.17340082]]\n",
      "last_rows\n",
      "[[-7.8815317 ]\n",
      " [-8.25390709]\n",
      " [-9.13786165]\n",
      " [-7.84285434]\n",
      " [-9.17340082]]\n",
      "new_rows\n",
      "[[-11.00419468]\n",
      " [-11.49500791]\n",
      " [-12.95623603]\n",
      " [-14.05231087]\n",
      " [-12.58894815]]\n",
      "last_rows\n",
      "[[-11.00419468]\n",
      " [-11.49500791]\n",
      " [-12.95623603]\n",
      " [-14.05231087]\n",
      " [-12.58894815]]\n",
      "new_rows\n",
      "[[-12.88323711]\n",
      " [-13.14735078]\n",
      " [-12.03071981]\n",
      " [-10.37519997]\n",
      " [-10.14380543]]\n",
      "last_rows\n",
      "[[-12.88323711]\n",
      " [-13.14735078]\n",
      " [-12.03071981]\n",
      " [-10.37519997]\n",
      " [-10.14380543]]\n",
      "new_rows\n",
      "[[-10.53786159]\n",
      " [ -9.60195392]\n",
      " [-10.0891093 ]\n",
      " [-10.31500962]\n",
      " [-10.34706015]]\n",
      "last_rows\n",
      "[[-10.53786159]\n",
      " [ -9.60195392]\n",
      " [-10.0891093 ]\n",
      " [-10.31500962]\n",
      " [-10.34706015]]\n",
      "new_rows\n",
      "[[ -9.80319457]\n",
      " [ -8.89336788]\n",
      " [ -8.99586806]\n",
      " [ -9.34652256]\n",
      " [-10.40003602]]\n",
      "last_rows\n",
      "[[ -9.80319457]\n",
      " [ -8.89336788]\n",
      " [ -8.99586806]\n",
      " [ -9.34652256]\n",
      " [-10.40003602]]\n",
      "new_rows\n",
      "[[-8.52825181]\n",
      " [-7.33688051]\n",
      " [-7.99615892]\n",
      " [-8.04012102]\n",
      " [-8.81500668]]\n",
      "last_rows\n",
      "[[-8.52825181]\n",
      " [-7.33688051]\n",
      " [-7.99615892]\n",
      " [-8.04012102]\n",
      " [-8.81500668]]\n",
      "new_rows\n",
      "[[ -9.54821005]\n",
      " [-11.00956545]\n",
      " [-13.14321785]\n",
      " [-12.36646389]\n",
      " [-12.91921215]]\n",
      "last_rows\n",
      "[[ -9.54821005]\n",
      " [-11.00956545]\n",
      " [-13.14321785]\n",
      " [-12.36646389]\n",
      " [-12.91921215]]\n",
      "new_rows\n",
      "[[-13.21731291]\n",
      " [-13.83737858]\n",
      " [-14.45293697]\n",
      " [-14.18335021]\n",
      " [-14.4923022 ]]\n",
      "last_rows\n",
      "[[-13.21731291]\n",
      " [-13.83737858]\n",
      " [-14.45293697]\n",
      " [-14.18335021]\n",
      " [-14.4923022 ]]\n",
      "new_rows\n",
      "[[-15.99179503]\n",
      " [-16.32296365]\n",
      " [-15.89158821]\n",
      " [-14.3670905 ]\n",
      " [-15.18515502]]\n",
      "last_rows\n",
      "[[-15.99179503]\n",
      " [-16.32296365]\n",
      " [-15.89158821]\n",
      " [-14.3670905 ]\n",
      " [-15.18515502]]\n",
      "new_rows\n",
      "[[-15.39857769]\n",
      " [-14.55212386]\n",
      " [-15.62323026]\n",
      " [-16.40961087]\n",
      " [-16.37593322]]\n",
      "last_rows\n",
      "[[-15.39857769]\n",
      " [-14.55212386]\n",
      " [-15.62323026]\n",
      " [-16.40961087]\n",
      " [-16.37593322]]\n",
      "new_rows\n",
      "[[-15.31695617]\n",
      " [-15.25023294]\n",
      " [-14.09564323]\n",
      " [-13.25231779]\n",
      " [-10.90945386]]\n",
      "last_rows\n",
      "[[-15.31695617]\n",
      " [-15.25023294]\n",
      " [-14.09564323]\n",
      " [-13.25231779]\n",
      " [-10.90945386]]\n",
      "new_rows\n",
      "[[-12.09277724]\n",
      " [-13.92751467]\n",
      " [-12.84995155]\n",
      " [-11.96881871]\n",
      " [-13.08253052]]\n",
      "last_rows\n",
      "[[-12.09277724]\n",
      " [-13.92751467]\n",
      " [-12.84995155]\n",
      " [-11.96881871]\n",
      " [-13.08253052]]\n",
      "new_rows\n",
      "[[-13.75950364]\n",
      " [-15.05166615]\n",
      " [-17.863779  ]\n",
      " [-18.6575812 ]\n",
      " [-16.71417823]]\n",
      "last_rows\n",
      "[[-13.75950364]\n",
      " [-15.05166615]\n",
      " [-17.863779  ]\n",
      " [-18.6575812 ]\n",
      " [-16.71417823]]\n",
      "new_rows\n",
      "[[-17.14510631]\n",
      " [-15.9738611 ]\n",
      " [-15.9085738 ]\n",
      " [-15.82508154]\n",
      " [-16.52526573]]\n",
      "last_rows\n",
      "[[-17.14510631]\n",
      " [-15.9738611 ]\n",
      " [-15.9085738 ]\n",
      " [-15.82508154]\n",
      " [-16.52526573]]\n",
      "new_rows\n",
      "[[-17.18847412]\n",
      " [-16.88240613]\n",
      " [-17.46214082]\n",
      " [-16.89291579]\n",
      " [-15.89329928]]\n",
      "last_rows\n",
      "[[-17.18847412]\n",
      " [-16.88240613]\n",
      " [-17.46214082]\n",
      " [-16.89291579]\n",
      " [-15.89329928]]\n",
      "new_rows\n",
      "[[-16.42851647]\n",
      " [-18.24583964]\n",
      " [-18.42168906]\n",
      " [-16.90434217]\n",
      " [-16.40649052]]\n",
      "last_rows\n",
      "[[-16.42851647]\n",
      " [-18.24583964]\n",
      " [-18.42168906]\n",
      " [-16.90434217]\n",
      " [-16.40649052]]\n",
      "new_rows\n",
      "[[-15.86434244]\n",
      " [-16.32975016]\n",
      " [-16.61197469]\n",
      " [-16.91761756]\n",
      " [-18.19921166]]\n",
      "last_rows\n",
      "[[-15.86434244]\n",
      " [-16.32975016]\n",
      " [-16.61197469]\n",
      " [-16.91761756]\n",
      " [-18.19921166]]\n",
      "new_rows\n",
      "[[-17.98671561]\n",
      " [-17.94179401]\n",
      " [-19.31145839]\n",
      " [-18.58585459]\n",
      " [-20.44228624]]\n",
      "last_rows\n",
      "[[-17.98671561]\n",
      " [-17.94179401]\n",
      " [-19.31145839]\n",
      " [-18.58585459]\n",
      " [-20.44228624]]\n",
      "new_rows\n",
      "[[-20.79407727]\n",
      " [-21.47184543]\n",
      " [-21.86951584]\n",
      " [-22.09039486]\n",
      " [-22.14083536]]\n",
      "last_rows\n",
      "[[-20.79407727]\n",
      " [-21.47184543]\n",
      " [-21.86951584]\n",
      " [-22.09039486]\n",
      " [-22.14083536]]\n",
      "new_rows\n",
      "[[-20.96745549]\n",
      " [-21.26519177]\n",
      " [-19.08287313]\n",
      " [-18.77894948]\n",
      " [-19.44852149]]\n",
      "last_rows\n",
      "[[-20.96745549]\n",
      " [-21.26519177]\n",
      " [-19.08287313]\n",
      " [-18.77894948]\n",
      " [-19.44852149]]\n",
      "new_rows\n",
      "[[-19.80592527]\n",
      " [-20.1539718 ]\n",
      " [-18.96065127]\n",
      " [-19.98386698]\n",
      " [-20.18849379]]\n",
      "last_rows\n",
      "[[-19.80592527]\n",
      " [-20.1539718 ]\n",
      " [-18.96065127]\n",
      " [-19.98386698]\n",
      " [-20.18849379]]\n",
      "new_rows\n",
      "[[-22.02636371]\n",
      " [-22.0752219 ]\n",
      " [-20.63522367]\n",
      " [-19.35925246]\n",
      " [-21.74271876]]\n",
      "last_rows\n",
      "[[-22.02636371]\n",
      " [-22.0752219 ]\n",
      " [-20.63522367]\n",
      " [-19.35925246]\n",
      " [-21.74271876]]\n",
      "new_rows\n",
      "[[-19.21996569]\n",
      " [-19.63586622]\n",
      " [-18.58373597]\n",
      " [-17.56337095]\n",
      " [-17.42281062]]\n",
      "last_rows\n",
      "[[-19.21996569]\n",
      " [-19.63586622]\n",
      " [-18.58373597]\n",
      " [-17.56337095]\n",
      " [-17.42281062]]\n",
      "new_rows\n",
      "[[-19.16398972]\n",
      " [-18.18510726]\n",
      " [-18.51040909]\n",
      " [-19.78042132]\n",
      " [-19.92145007]]\n",
      "last_rows\n",
      "[[-19.16398972]\n",
      " [-18.18510726]\n",
      " [-18.51040909]\n",
      " [-19.78042132]\n",
      " [-19.92145007]]\n",
      "new_rows\n",
      "[[-19.41243869]\n",
      " [-20.27234457]\n",
      " [-20.29133801]\n",
      " [-21.12931274]\n",
      " [-20.89460474]]\n",
      "last_rows\n",
      "[[-19.41243869]\n",
      " [-20.27234457]\n",
      " [-20.29133801]\n",
      " [-21.12931274]\n",
      " [-20.89460474]]\n",
      "new_rows\n",
      "[[-20.62968355]\n",
      " [-22.10870047]\n",
      " [-21.42474863]\n",
      " [-20.46956535]\n",
      " [-20.48169084]]\n",
      "last_rows\n",
      "[[-20.62968355]\n",
      " [-22.10870047]\n",
      " [-21.42474863]\n",
      " [-20.46956535]\n",
      " [-20.48169084]]\n",
      "new_rows\n",
      "[[-19.37317585]\n",
      " [-19.52149404]\n",
      " [-19.17531421]\n",
      " [-19.91419148]\n",
      " [-19.976725  ]]\n",
      "last_rows\n",
      "[[-19.37317585]\n",
      " [-19.52149404]\n",
      " [-19.17531421]\n",
      " [-19.91419148]\n",
      " [-19.976725  ]]\n",
      "new_rows\n",
      "[[-20.88971926]\n",
      " [-17.96507764]\n",
      " [-19.14992373]\n",
      " [-20.65088643]\n",
      " [-19.11964502]]\n",
      "last_rows\n",
      "[[-20.88971926]\n",
      " [-17.96507764]\n",
      " [-19.14992373]\n",
      " [-20.65088643]\n",
      " [-19.11964502]]\n",
      "new_rows\n",
      "[[-19.67286013]\n",
      " [-18.12641336]\n",
      " [-18.02831279]\n",
      " [-17.54488854]\n",
      " [-15.7928603 ]]\n",
      "last_rows\n",
      "[[-19.67286013]\n",
      " [-18.12641336]\n",
      " [-18.02831279]\n",
      " [-17.54488854]\n",
      " [-15.7928603 ]]\n",
      "new_rows\n",
      "[[-16.13690936]\n",
      " [-16.36598103]\n",
      " [-16.67047087]\n",
      " [-15.64659713]\n",
      " [-15.5187286 ]]\n",
      "last_rows\n",
      "[[-16.13690936]\n",
      " [-16.36598103]\n",
      " [-16.67047087]\n",
      " [-15.64659713]\n",
      " [-15.5187286 ]]\n",
      "new_rows\n",
      "[[-16.88371605]\n",
      " [-14.43029226]\n",
      " [-14.57089559]\n",
      " [-14.6824238 ]\n",
      " [-14.026539  ]]\n",
      "last_rows\n",
      "[[-16.88371605]\n",
      " [-14.43029226]\n",
      " [-14.57089559]\n",
      " [-14.6824238 ]\n",
      " [-14.026539  ]]\n",
      "new_rows\n",
      "[[-15.27202094]\n",
      " [-16.23147559]\n",
      " [-16.43914465]\n",
      " [-15.96476641]\n",
      " [-17.14096743]]\n",
      "last_rows\n",
      "[[-15.27202094]\n",
      " [-16.23147559]\n",
      " [-16.43914465]\n",
      " [-15.96476641]\n",
      " [-17.14096743]]\n",
      "new_rows\n",
      "[[-17.39890473]\n",
      " [-17.30640867]\n",
      " [-17.13792885]\n",
      " [-16.53076349]\n",
      " [-18.68808273]]\n",
      "last_rows\n",
      "[[-17.39890473]\n",
      " [-17.30640867]\n",
      " [-17.13792885]\n",
      " [-16.53076349]\n",
      " [-18.68808273]]\n",
      "new_rows\n",
      "[[-19.20643417]\n",
      " [-17.88806925]\n",
      " [-16.72814716]\n",
      " [-15.61753232]\n",
      " [-14.96879156]]\n",
      "last_rows\n",
      "[[-19.20643417]\n",
      " [-17.88806925]\n",
      " [-16.72814716]\n",
      " [-15.61753232]\n",
      " [-14.96879156]]\n",
      "new_rows\n",
      "[[-15.65375692]\n",
      " [-15.14016596]\n",
      " [-14.43936823]\n",
      " [-14.15587117]\n",
      " [-14.14775823]]\n",
      "last_rows\n",
      "[[-15.65375692]\n",
      " [-15.14016596]\n",
      " [-14.43936823]\n",
      " [-14.15587117]\n",
      " [-14.14775823]]\n",
      "new_rows\n",
      "[[-13.85350172]\n",
      " [-14.20460732]\n",
      " [-12.27089865]\n",
      " [ -9.61314003]\n",
      " [ -8.99773223]]\n",
      "last_rows\n",
      "[[-13.85350172]\n",
      " [-14.20460732]\n",
      " [-12.27089865]\n",
      " [ -9.61314003]\n",
      " [ -8.99773223]]\n",
      "new_rows\n",
      "[[ -9.31749907]\n",
      " [ -9.60661913]\n",
      " [-11.83534915]\n",
      " [-13.10535941]\n",
      " [-13.71011732]]\n",
      "last_rows\n",
      "[[ -9.31749907]\n",
      " [ -9.60661913]\n",
      " [-11.83534915]\n",
      " [-13.10535941]\n",
      " [-13.71011732]]\n",
      "new_rows\n",
      "[[-13.62639042]\n",
      " [-12.43586066]\n",
      " [-13.48900027]\n",
      " [-12.47258443]\n",
      " [-11.43249876]]\n",
      "last_rows\n",
      "[[-13.62639042]\n",
      " [-12.43586066]\n",
      " [-13.48900027]\n",
      " [-12.47258443]\n",
      " [-11.43249876]]\n",
      "new_rows\n",
      "[[-11.07651454]\n",
      " [-13.014738  ]\n",
      " [-14.09432893]\n",
      " [-13.25028379]\n",
      " [-13.79915184]]\n",
      "last_rows\n",
      "[[-11.07651454]\n",
      " [-13.014738  ]\n",
      " [-14.09432893]\n",
      " [-13.25028379]\n",
      " [-13.79915184]]\n",
      "new_rows\n",
      "[[-13.96172998]\n",
      " [-14.15168302]\n",
      " [-14.50540797]\n",
      " [-13.98869217]\n",
      " [-13.80222852]]\n",
      "last_rows\n",
      "[[-13.96172998]\n",
      " [-14.15168302]\n",
      " [-14.50540797]\n",
      " [-13.98869217]\n",
      " [-13.80222852]]\n",
      "new_rows\n",
      "[[-13.69724659]\n",
      " [-12.67879049]\n",
      " [-11.46605276]\n",
      " [-11.31767322]\n",
      " [-10.95990098]]\n",
      "last_rows\n",
      "[[-13.69724659]\n",
      " [-12.67879049]\n",
      " [-11.46605276]\n",
      " [-11.31767322]\n",
      " [-10.95990098]]\n",
      "new_rows\n",
      "[[-10.37431842]\n",
      " [-10.78257988]\n",
      " [-12.17106672]\n",
      " [-12.74482712]\n",
      " [-12.12731311]]\n",
      "last_rows\n",
      "[[-10.37431842]\n",
      " [-10.78257988]\n",
      " [-12.17106672]\n",
      " [-12.74482712]\n",
      " [-12.12731311]]\n",
      "new_rows\n",
      "[[ -9.97991935]\n",
      " [-11.17423972]\n",
      " [-12.25578061]\n",
      " [-11.36045269]\n",
      " [-11.10744975]]\n",
      "last_rows\n",
      "[[ -9.97991935]\n",
      " [-11.17423972]\n",
      " [-12.25578061]\n",
      " [-11.36045269]\n",
      " [-11.10744975]]\n",
      "new_rows\n",
      "[[-12.96691023]\n",
      " [-11.1586167 ]\n",
      " [-10.80804724]\n",
      " [-10.50773228]\n",
      " [-12.97549805]]\n",
      "last_rows\n",
      "[[-12.96691023]\n",
      " [-11.1586167 ]\n",
      " [-10.80804724]\n",
      " [-10.50773228]\n",
      " [-12.97549805]]\n",
      "new_rows\n",
      "[[-12.77239324]\n",
      " [-10.88724514]\n",
      " [ -9.46045701]\n",
      " [ -8.91144115]\n",
      " [ -8.46136099]]\n",
      "last_rows\n",
      "[[-12.77239324]\n",
      " [-10.88724514]\n",
      " [ -9.46045701]\n",
      " [ -8.91144115]\n",
      " [ -8.46136099]]\n",
      "new_rows\n",
      "[[ -9.53931963]\n",
      " [ -8.92900199]\n",
      " [-11.39155776]\n",
      " [-11.12357534]\n",
      " [-10.99268678]]\n",
      "last_rows\n",
      "[[ -9.53931963]\n",
      " [ -8.92900199]\n",
      " [-11.39155776]\n",
      " [-11.12357534]\n",
      " [-10.99268678]]\n",
      "new_rows\n",
      "[[-10.10852857]\n",
      " [-10.81996411]\n",
      " [-12.63856012]\n",
      " [-12.64441601]\n",
      " [-12.94951523]]\n",
      "last_rows\n",
      "[[-10.10852857]\n",
      " [-10.81996411]\n",
      " [-12.63856012]\n",
      " [-12.64441601]\n",
      " [-12.94951523]]\n",
      "new_rows\n",
      "[[-13.40514055]\n",
      " [-13.41656649]\n",
      " [-12.57344485]\n",
      " [-12.73048167]\n",
      " [-11.92919596]]\n",
      "last_rows\n",
      "[[-13.40514055]\n",
      " [-13.41656649]\n",
      " [-12.57344485]\n",
      " [-12.73048167]\n",
      " [-11.92919596]]\n",
      "new_rows\n",
      "[[-12.72685171]\n",
      " [-11.88536994]\n",
      " [ -9.98608326]\n",
      " [-11.27520156]\n",
      " [-12.72246413]]\n",
      "last_rows\n",
      "[[-12.72685171]\n",
      " [-11.88536994]\n",
      " [ -9.98608326]\n",
      " [-11.27520156]\n",
      " [-12.72246413]]\n",
      "new_rows\n",
      "[[-12.34960706]\n",
      " [-11.44225298]\n",
      " [-12.25219622]\n",
      " [-14.22783126]\n",
      " [-13.98575122]]\n",
      "last_rows\n",
      "[[-12.34960706]\n",
      " [-11.44225298]\n",
      " [-12.25219622]\n",
      " [-14.22783126]\n",
      " [-13.98575122]]\n",
      "new_rows\n",
      "[[-12.67261848]\n",
      " [-15.44449457]\n",
      " [-14.40142691]\n",
      " [-15.46178133]\n",
      " [-16.47475276]]\n",
      "last_rows\n",
      "[[-12.67261848]\n",
      " [-15.44449457]\n",
      " [-14.40142691]\n",
      " [-15.46178133]\n",
      " [-16.47475276]]\n",
      "new_rows\n",
      "[[-16.25333462]\n",
      " [-16.21707933]\n",
      " [-16.0156945 ]\n",
      " [-16.60931045]\n",
      " [-17.836369  ]]\n",
      "last_rows\n",
      "[[-16.25333462]\n",
      " [-16.21707933]\n",
      " [-16.0156945 ]\n",
      " [-16.60931045]\n",
      " [-17.836369  ]]\n",
      "new_rows\n",
      "[[-18.24185576]\n",
      " [-18.50595689]\n",
      " [-19.47470874]\n",
      " [-20.58176775]\n",
      " [-20.30523459]]\n",
      "last_rows\n",
      "[[-18.24185576]\n",
      " [-18.50595689]\n",
      " [-19.47470874]\n",
      " [-20.58176775]\n",
      " [-20.30523459]]\n",
      "new_rows\n",
      "[[-20.6671912 ]\n",
      " [-19.57967697]\n",
      " [-19.26230945]\n",
      " [-18.63375481]\n",
      " [-17.32363754]]\n",
      "last_rows\n",
      "[[-20.6671912 ]\n",
      " [-19.57967697]\n",
      " [-19.26230945]\n",
      " [-18.63375481]\n",
      " [-17.32363754]]\n",
      "new_rows\n",
      "[[-16.98127644]\n",
      " [-16.73440755]\n",
      " [-16.7022782 ]\n",
      " [-17.46233962]\n",
      " [-16.12953955]]\n",
      "last_rows\n",
      "[[-16.98127644]\n",
      " [-16.73440755]\n",
      " [-16.7022782 ]\n",
      " [-17.46233962]\n",
      " [-16.12953955]]\n",
      "new_rows\n",
      "[[-16.20178444]\n",
      " [-13.7156379 ]\n",
      " [-13.63545192]\n",
      " [-13.812996  ]\n",
      " [-13.35470788]]\n",
      "last_rows\n",
      "[[-16.20178444]\n",
      " [-13.7156379 ]\n",
      " [-13.63545192]\n",
      " [-13.812996  ]\n",
      " [-13.35470788]]\n",
      "new_rows\n",
      "[[-13.75634952]\n",
      " [-14.23686793]\n",
      " [-12.95720903]\n",
      " [-12.84654413]\n",
      " [-13.92071394]]\n",
      "last_rows\n",
      "[[-13.75634952]\n",
      " [-14.23686793]\n",
      " [-12.95720903]\n",
      " [-12.84654413]\n",
      " [-13.92071394]]\n",
      "new_rows\n",
      "[[-12.99354933]\n",
      " [-13.37281739]\n",
      " [-15.33473297]\n",
      " [-16.08575677]\n",
      " [-16.56340275]]\n",
      "last_rows\n",
      "[[-12.99354933]\n",
      " [-13.37281739]\n",
      " [-15.33473297]\n",
      " [-16.08575677]\n",
      " [-16.56340275]]\n",
      "new_rows\n",
      "[[-18.21201215]\n",
      " [-17.56799915]\n",
      " [-17.91632239]\n",
      " [-17.52479379]\n",
      " [-17.45116277]]\n",
      "last_rows\n",
      "[[-18.21201215]\n",
      " [-17.56799915]\n",
      " [-17.91632239]\n",
      " [-17.52479379]\n",
      " [-17.45116277]]\n",
      "new_rows\n",
      "[[-17.99088487]\n",
      " [-17.09628537]\n",
      " [-15.17195074]\n",
      " [-13.93806148]\n",
      " [-15.85386377]]\n",
      "last_rows\n",
      "[[-17.99088487]\n",
      " [-17.09628537]\n",
      " [-15.17195074]\n",
      " [-13.93806148]\n",
      " [-15.85386377]]\n",
      "new_rows\n",
      "[[-15.6296141 ]\n",
      " [-15.84636312]\n",
      " [-15.43108974]\n",
      " [-15.7326216 ]\n",
      " [-17.35408588]]\n",
      "last_rows\n",
      "[[-15.6296141 ]\n",
      " [-15.84636312]\n",
      " [-15.43108974]\n",
      " [-15.7326216 ]\n",
      " [-17.35408588]]\n",
      "new_rows\n",
      "[[-19.06823656]\n",
      " [-19.84158462]\n",
      " [-19.33068688]\n",
      " [-19.7112899 ]\n",
      " [-19.30116292]]\n",
      "last_rows\n",
      "[[-19.06823656]\n",
      " [-19.84158462]\n",
      " [-19.33068688]\n",
      " [-19.7112899 ]\n",
      " [-19.30116292]]\n",
      "new_rows\n",
      "[[-19.64944914]\n",
      " [-20.283708  ]\n",
      " [-22.02398529]\n",
      " [-21.22984972]\n",
      " [-21.11361767]]\n",
      "last_rows\n",
      "[[-19.64944914]\n",
      " [-20.283708  ]\n",
      " [-22.02398529]\n",
      " [-21.22984972]\n",
      " [-21.11361767]]\n",
      "new_rows\n",
      "[[-20.30287132]\n",
      " [-19.23209155]\n",
      " [-18.07192193]\n",
      " [-17.60031105]\n",
      " [-17.42661793]]\n",
      "last_rows\n",
      "[[-20.30287132]\n",
      " [-19.23209155]\n",
      " [-18.07192193]\n",
      " [-17.60031105]\n",
      " [-17.42661793]]\n",
      "new_rows\n",
      "[[-16.9509316 ]\n",
      " [-17.35670444]\n",
      " [-18.32769621]\n",
      " [-19.42863045]\n",
      " [-20.26913932]]\n",
      "last_rows\n",
      "[[-16.9509316 ]\n",
      " [-17.35670444]\n",
      " [-18.32769621]\n",
      " [-19.42863045]\n",
      " [-20.26913932]]\n",
      "new_rows\n",
      "[[-20.96537013]\n",
      " [-20.91550246]\n",
      " [-20.68005016]\n",
      " [-19.93278807]\n",
      " [-20.33834771]]\n",
      "last_rows\n",
      "[[-20.96537013]\n",
      " [-20.91550246]\n",
      " [-20.68005016]\n",
      " [-19.93278807]\n",
      " [-20.33834771]]\n",
      "new_rows\n",
      "[[-21.99232653]\n",
      " [-20.3146689 ]\n",
      " [-20.43214952]\n",
      " [-22.79829259]\n",
      " [-22.87380692]]\n",
      "last_rows\n",
      "[[-21.99232653]\n",
      " [-20.3146689 ]\n",
      " [-20.43214952]\n",
      " [-22.79829259]\n",
      " [-22.87380692]]\n",
      "new_rows\n",
      "[[-21.49208138]\n",
      " [-21.9502119 ]\n",
      " [-22.70759418]\n",
      " [-22.3275305 ]\n",
      " [-21.36192366]]\n",
      "last_rows\n",
      "[[-21.49208138]\n",
      " [-21.9502119 ]\n",
      " [-22.70759418]\n",
      " [-22.3275305 ]\n",
      " [-21.36192366]]\n",
      "new_rows\n",
      "[[-21.43825   ]\n",
      " [-18.29550877]\n",
      " [-17.97996191]\n",
      " [-17.53827019]\n",
      " [-16.23287351]]\n",
      "last_rows\n",
      "[[-21.43825   ]\n",
      " [-18.29550877]\n",
      " [-17.97996191]\n",
      " [-17.53827019]\n",
      " [-16.23287351]]\n",
      "new_rows\n",
      "[[-15.21367813]\n",
      " [-14.24202623]\n",
      " [-13.22184102]\n",
      " [-14.24642858]\n",
      " [-15.33478622]]\n",
      "last_rows\n",
      "[[-15.21367813]\n",
      " [-14.24202623]\n",
      " [-13.22184102]\n",
      " [-14.24642858]\n",
      " [-15.33478622]]\n",
      "new_rows\n",
      "[[-16.52180309]\n",
      " [-17.86726594]\n",
      " [-16.67572262]\n",
      " [-17.30540824]\n",
      " [-17.16022601]]\n",
      "last_rows\n",
      "[[-16.52180309]\n",
      " [-17.86726594]\n",
      " [-16.67572262]\n",
      " [-17.30540824]\n",
      " [-17.16022601]]\n",
      "new_rows\n",
      "[[-15.88707547]\n",
      " [-15.69790452]\n",
      " [-14.70726959]\n",
      " [-12.7319338 ]\n",
      " [-13.75869619]]\n",
      "last_rows\n",
      "[[-15.88707547]\n",
      " [-15.69790452]\n",
      " [-14.70726959]\n",
      " [-12.7319338 ]\n",
      " [-13.75869619]]\n",
      "new_rows\n",
      "[[-14.45239932]\n",
      " [-15.12851093]\n",
      " [-14.93721754]\n",
      " [-14.21831303]\n",
      " [-16.04888821]]\n",
      "last_rows\n",
      "[[-14.45239932]\n",
      " [-15.12851093]\n",
      " [-14.93721754]\n",
      " [-14.21831303]\n",
      " [-16.04888821]]\n",
      "new_rows\n",
      "[[-17.70834504]\n",
      " [-17.1036523 ]\n",
      " [-17.43245105]\n",
      " [-18.115852  ]\n",
      " [-16.9965199 ]]\n",
      "last_rows\n",
      "[[-17.70834504]\n",
      " [-17.1036523 ]\n",
      " [-17.43245105]\n",
      " [-18.115852  ]\n",
      " [-16.9965199 ]]\n",
      "new_rows\n",
      "[[-16.60038918]\n",
      " [-17.35572348]\n",
      " [-17.01975842]\n",
      " [-16.10811006]\n",
      " [-14.87969944]]\n",
      "last_rows\n",
      "[[-16.60038918]\n",
      " [-17.35572348]\n",
      " [-17.01975842]\n",
      " [-16.10811006]\n",
      " [-14.87969944]]\n",
      "new_rows\n",
      "[[-15.65337422]\n",
      " [-15.15386181]\n",
      " [-12.67775915]\n",
      " [-12.75220693]\n",
      " [-14.47431339]]\n",
      "last_rows\n",
      "[[-15.65337422]\n",
      " [-15.15386181]\n",
      " [-12.67775915]\n",
      " [-12.75220693]\n",
      " [-14.47431339]]\n",
      "new_rows\n",
      "[[-13.88975297]\n",
      " [-13.27957078]\n",
      " [-10.55674188]\n",
      " [ -9.38033643]\n",
      " [ -9.23353891]]\n",
      "last_rows\n",
      "[[-13.88975297]\n",
      " [-13.27957078]\n",
      " [-10.55674188]\n",
      " [ -9.38033643]\n",
      " [ -9.23353891]]\n",
      "new_rows\n",
      "[[-9.85031135]\n",
      " [-8.66316135]\n",
      " [-8.18598495]\n",
      " [-7.95333919]\n",
      " [-7.16961809]]\n",
      "last_rows\n",
      "[[-9.85031135]\n",
      " [-8.66316135]\n",
      " [-8.18598495]\n",
      " [-7.95333919]\n",
      " [-7.16961809]]\n",
      "new_rows\n",
      "[[-6.82278641]\n",
      " [-7.03514515]\n",
      " [-5.95484113]\n",
      " [-4.91334255]\n",
      " [-5.34235334]]\n",
      "last_rows\n",
      "[[-6.82278641]\n",
      " [-7.03514515]\n",
      " [-5.95484113]\n",
      " [-4.91334255]\n",
      " [-5.34235334]]\n",
      "new_rows\n",
      "[[-5.38413659]\n",
      " [-5.22094393]\n",
      " [-5.20869407]\n",
      " [-4.83339389]\n",
      " [-3.37493994]]\n",
      "last_rows\n",
      "[[-5.38413659]\n",
      " [-5.22094393]\n",
      " [-5.20869407]\n",
      " [-4.83339389]\n",
      " [-3.37493994]]\n",
      "new_rows\n",
      "[[-3.23594386]\n",
      " [-1.40055373]\n",
      " [-2.31637799]\n",
      " [-3.50499846]\n",
      " [-5.37673846]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "last_rows = np.random.randn(1, 1)\n",
    "\n",
    "for i in range(1, 101):\n",
    "    print('last_rows')\n",
    "    print(last_rows)\n",
    "    new_rows = last_rows[-1, :] + np.random.randn(5, 1).cumsum(axis=0)\n",
    "    print('new_rows')\n",
    "    print(new_rows)\n",
    "    #status_text.text(\"%i%% Complete\" % i)\n",
    "    #chart.add_rows(new_rows)\n",
    "    #progress_bar.progress(i)\n",
    "    last_rows = new_rows\n",
    "    #time.sleep(0.05)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2f5119",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
